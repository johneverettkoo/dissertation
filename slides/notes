Thanks for coming to my dissertation defense. 
The topic of my dissertation is community detection in the setting of generalized random dot product graphs.

This presentation contains three sections. 
First, I'll go through some preliminaries and introduce two families of random graph models and how they are related. 
The second section is about the relationship between two types of random graphs, namely how the popularity adjusted block model is a generalized random dot product graph, and how that relationship can be exploited for community detection.
Third, I'll end by introducing a new type of random graph model, why it is of interest, and how we could perform statistical inference on this model.

So, onto the preliminaries of random graph models. 
Suppose we observe a graph with community structure. 
In this example is a friendship network among faculty members at a university in the UK. 
The vertices represent the individual faculty and the edges represent whether pairs of faculty are friends. 
This dataset also comes with school affiliation for each faculty member, but let's say we do not know this information and we want to be able to infer it. 
This type of problem is called graph clustering or community detection. 

To perform this task within the context of statistical inference, we first need to define a random graph model. 
One such type of model for undirected and unweighted graphs is called the bernoulli random graph. 
We describe these graphs by an n by n adjacency matrix A, where n is the number of vertices, and A contains entries 1 where there is an edge between its corresponding pair of vertices and 0 otherwise.
We say A describes a bernoulli random graph if there is a corresponding n by n edge probability matrix P such that each A_ij is an independent bernoulli trial with P_ij.

Let's build on this to graphs with community structure. 
We begin by assigning to each vertex of the graph community label z_i which can take on values from 1 to K. 
Then if each P_ij is a function of z_i and z_j, along with possibly some other parameters specific to vertices i and j, we call this a block model. 
The simpliest and most popular example of this is the stochastic block model, which has a fixed edge probability for each pair of communities. 
In this particular example, we have two communities, so K = 2 and each z_i can take on values 1 or 2. 
Then we set each P_ij as p if both i and j are in community 1, q if they are both in community 2, or r if they are in different communities. 

The SBM is described succinctly by the parameters theta_kl, which are the community-wise edge probabilities.
Each P_ij is theta_{z_i,z_j}. 
The SBM is a fairly rigid model, so there have been various generalizations of it developed over the years, and now we have a family of block models to choose from when performing statistical inference. 
The degree corrected block model supposes that in addition to community-wise edge probabilities theta_kl, each vertex has a degree correction term omega_i that adjusts its expected degree. 
Then the edge probability between vertices i and j is the community-wise edge probability theta_{z_i, z_j} times omega_i, the i^th vertex's degree correction factor, times omega_j, the j^th vertex's degree correction factor. 
The popularity adjusted block model takes this further and allows each vertex to have its own affinity toward each community. 
Each vertex has K popularity parameters, each describing how likely that vertex is to connect with vertices from each of the K communities. 
In the PABM, the edge probability between vertices i and j is vertex i's popularity parameter toward vertex j's community times vertex j's popularity parameter toward vertex i's community. 

To illustrate the three block models, the SBM, DCBM, and PABM, and how they are successive generalizations, here is an example. 

First, we begin with the SBM. 
Suppose that we use a graph to describe an online dating service. 
In this graph, each vertex is a user of the service and each edge is whether pairs of users were successfully matched by the service. 
To treat this as a block model, we use the users' genders as communities. 
For the sake of making this a simple example, we'll restrict our case to female and male for now. 
To model this as an SBM, we might say that there is a fixed probability of a match between two female members, another fixed probability of a match between two male members, and a third fixed probability of a match between a female and a male member. 
Also worth noting is that in this example, this model is disassortative. 
There is a higher probability of a match between members of opposite genders than between members of the same gender. 
One problem with this model is that it assumes that each user of the service is equally popular with other users of each gender. 
In reality, some users are very popular and make many connections while others are less so and make few or no connections. 

The DCBM allows us to model each user's popularity with other users via the degree correction factor omega_i. 
If user i is not very popular, then they may have a low omega_i, or if they are popular and make many connections on the service, they may have a high omega_i. 

However, the DCBM is unable to model each user's sexual orientation. 
If we model this as a PABM, then each user has two popularity parameters that model that user's likelihood of connecting with members of each gender. 

This slide illustrates one way in which the SBM, DCBM, and PABM are successive generalizations. 
One thing to note from this is that in the SBM and DCBM, the edge probability matrix P is rank K, whereas in the PABM it is rank K^2. 
This comes into play for estimation later on. 

Speaking of estimation, since these are statistical models, one approach we might take is likelihood maximization. 
However, block models are mixture models, so direct likelihood maximization is not feasible. 
One approach to likelihood maximization for mixture models is expectation maximization, but in this case, the mixture term in the log likelihood makes this difficult. 
We can impose an independence relaxation, so we say E[z_ik z_jl] = E[z_ik] x E[z_jl], which leads to this algorithm. 
It's not clear whether this relaxation is problematic, but in numerical experiments, this algorithm appears to work well. 
Note that while the SBM is a fairly simple model, this algorithm is pretty clunky, which is why no one really does this in practice. 
This type of algorithm can be adapted for bayesian methods such as variational inference, or sometimes alternative methods such as graph embedding is used. 
The main takeaway here is that we have this family of block models that are all fairly simple as generative models and are plausible models for a wide range of networks, but estimation is difficult. 

Now we'll consider another family of bernoulli random graph models called the generalized random dot product graph. 
We say that A is sampled from a generalized random dot product graph if each vertex has a corresponding vector in some latent space and the edge probability between vertices i and j as the indefinite inner product between latent vectors x_i and x_j, where the indefinite inner product is characterized by Ipq, which is a diagonal matrix of p positive ones and q negative ones. Then our edge probability matrix is X Ipq X^top. 

Under this model, an inference task might be to estimate the latent positions from one observation of the graph described by adjacency matrix A. 
The adjacency spectral embedding does this by taking the spectral decomposition of A, and it has already been shown that the ASE asymoptotically approaches the true latent positions up to an indefinite orthogonal transformation. 

So now we have two different models for graphs, one in which edges are drawn based on community structure and another where edges are drawn based on vectors in some latent space. 

Now, we'll show how these two families of models are connected and how that connection is useful for estimation. 
It can be easily shown that all bernoulli graphs are generalized random dot product graphs. 
P can be decomposed, and that decomposition can describe vectors in latent space. 
To illustrate this more explicitly, let's return to the stochastic block model example from before. If we organize P by community, the upper left block is all p, the lower right is q, and the off diagonal blocks are r. 
Then it's easy to find a two dimensional set of latent vectors that multiply to P.
Furthermore, the latent vectors lie on just two point masses, one at (sqrt(p), 0) and another at (sqrt(r^2 / p), sqrt(q - r^2 / p)). 

This leads to a straightforward community detection method. 
We just take the ASE of A, and we know that as n increases, the ASE converges to the latent configuration, that is, two point masses which correspond to the two communities, and we can apply a clustering technique on the ASE that is based on clusters aggregating around point masses, such as K-means or gaussian mixture models. 

Extending this to the DCBM, we can transform P for the SBM by multiplying each P_ij by omega_i and omega_j. 
This is equivalent to left and right multiplying P by big Omega, which is a diagonal matrix of the omega_i's. 
Then it's straghtforward to see that each latent vector of the DCBM is the corresponding latent vector of the PABM multiplied by its degree correction factor. 
This in effect stretches out the two point masses toward the origin to form rays. 

Estimation for the DCBM then begins exactly the same way as in the SBM, but we can see that the embedding consists of two fuzzy line segments that meet at the origin instead of two fuzzy point masses. 
A plausible clustering method for this is K-means clustering with the cosine kernel. 

Viewing block models as GRDPGs isn't only just useful for inference but also for exploratory data analysis. 
Going back to our first example with the friendships among UK faculty members, just from looking at the graph, it is not clear whether an SBM or DCBM is more appropriate for this model. 
If we didn't have the community labels, then we also can't compare community detection errors from fitting an SBM or DCBM to this graph. 
However, treating this as a GRDPG, the ASE makes it abundantly clear that the DCBM is a more appropriate model for this network, based on the structure of the approximated latent vectors.
Fitting a DCBM by clustering on the ASE here results in  around 97% accuracy. 
Fitting a DCBM via an EM algorithm results in slightly lower 95% accuracy, but the main improvement on the GRDPG approach is that we can visually see that the DCBM is a good model, and the model fitting algorithm is much simpler. 

So far, we saw that the SBM and DCBM are GRDPGs and that connection is useful for community detection. 
This section's focus is on the connection between the PABM and GRDPG and the ways in which that connection is useful for statistical inference. 

In order to make this connection, we first reframe the PABM by describing P as being composed of K^2 popularity vectors. 
If we sort P by community such that the kl^th block is the n_k by n_l matrix of edge probabilities, then that block is comprised of the outer product of two popularity vectors.
Each popularity vector lambda^kl is the vector of popularity parameters belonging to vertices in community k toward community l. 
As a reminder, lambda_il is the i^th vertex's popularity parameter to community l.
If vertex i is in community k, then lambda_il would be one entry in the vector lambda^kl.
This result reveals that the edge probability matrix of a PABM is rank K^2 and has linear latent structure. 
As a quick reminder, the edge probability matrices for SBMs and DCBMs are rank K and also display linear latent structure. 

Like we were able to show for the SBM and DCBM, we can view the PABM as a GRDPG and define a latent structure that is organized by community. 
This latent structure is not unique, but the way in which we define it here is particularly useful. 
In short, the latent structure is in K^2 dimensions and and block diagonal with each block taking up K columns and corresponding to the K communities. 
This means that each community takes up a K-dimensional subspace in the K^2-dimensional space. 
Furthermore, these subspaces are mutually disjoint and orthogonal. 
More explicitly, if we define capital Lambda^k as the n-sub-little-k by capital-K matrix of popularity parameters belonging to vertices in community little-k and then collect those into an n by K^2 block diagonal matrix, there is a K^2 by K^2 orthogonal matrix U such that X U I_pq (XU)^T results in the edge probability matrix for a PABM with these popularity parameters. 

This block diagonal nature lends itself nicely to our first result, which is that if we decompose P and construct an n by n outer product matrix using the eigenvectors and call that B, then for each pair of vertices i and j belonging to two different communities, B_ij is 0. 
Note that this is a result for the edge probability matrix, not the adjacency matrix.
But the consistency of the adjacency spectral embedding suggests that this is approximately true, which leads us to orthogonal spectral clustering. 
This algorithm is pretty much applying this theorem on the eigenvectors of A instead of P. 
Because the eigenvectors of A only approximate the eigenvectors of P, the B_ij^th entry for vertices i and j belonging to different communities won't necessarily be exactly 0, so there needs to be a final graph partitioning step, such as edge thresholding. 

We were able to show that this algorithm is indeed consistent.
In particular, let B-hat_ij be the outer product matrix of eigenvectors from orthogonal spectral clustering, and i and j be the indices for a pair of vertices belonging to different communities. 
Then as n increases, the maximum size of B-hat_ij decreases with this rate, with high probabiity. 
Then for a large enough n, orthogonal spectral clustering results in zero community detection error with probability 1. 
Note that this is a statement on the number of mis-clustered vertices, not the mis-clustering error rate. 

Next, we'll take a look at another algorithm for community detection on the pabm that exploits the latent configuration. 
We already showed that the ASE of the PABM in K^2-dimensions consists of points lying near K K-dimensional subspaces. 
As it turns out, there already is a family of clustering algorithms for data that lie on or near subspaces. 
We'll focus on one particular one called sparse subspace clustering. 
This algorithm essentially amounts to fitting a sparse regression for each vector using the other vectors as covariates. 
The idea is that if x_i lives in a subspace with a few other vectors in X, then it can be expressed as a linear combination of them, with the other coefficients being 0. 
Then if we combine all of these coefficients into a matrix B, B_ij will be 0 for i and j on different subspaces.
In practice, the sparse regression fits are done via lasso, which requires us to choose the sparsity parameter lambda.

We weren't the first to propose the use of sparse subspace clustering for this problem. 
Noroozi, Rimal, and Pensky noticed that P is rank K^2 and the columns of P corresponding to each community are rank K to justify the use of SSC. 
They showed that if you treat P as a data matrix for SSC, then the output obeys the subspace detection property, the details of which we won't get into here, but the subspace detection property is considered a success criterion for ssc.
Our approach uses the geometry of the latent configuration to justify using ssc on the ASE. Actually, ssc works better the more orthogonal these subspaces are, and we saw that if we treat the rows of the eigenvector matrix as an embedding, we get exactly that. 

This leads to our version of the sparse subspace clustering algorithm for the pabm. 
First, we take the approximate spectral decomposition of A, then apply SSC on the rows of the eigenvector matrix. 
Then we were able to prove that for a large yet finite n, the rows of V obey the subspace detection property, provided that we choose the correct sparsity parameter. 
In some sense, this is a nicer result than the one for orthogonal spectral clustering, which has B_ij going to 0 asymptotically--for ssc, B_ij is exactly 0 if the conditions are just right. 

We also ran a few simulations to test our algorithms empirically. 
In this setup, the communities were drawn to be approximately equally sized, and the popularity parameters were drawn such that there are more within-community edges than between-community edges. 
Here, we show the interquartile range of the community detection error rates of our simulations, with the x axis being the size of the graph. 
There is some weird behavior going on for ssc with K = 2, but otherwise, all of these methods result in zero error for large n, with orthogonal spectral clustering and ssc on the eigenvectors performing better than ssc on the adjacency matrix.
Note that the y axis here is error count, not error rate, as in the total number of mis-clustered vertices. 

We modified this setup by drawing imbalanced communities, and again, we show error converging to zero for larger and larger graphs. 

Finally, we tried a setup in which the popularity parameters were drawn such that there is usually a higher probability of drawing an edge between communities than within communities, and we achieved similar results from orthogonal spectral clustering and sparse subspace clustering. 
Note that even though the graphs are disassortative, the application of OSC and SSC remain identical as in the assortative cases from before. 

We also analyzed three real graphs which were used as examples in the original PABM paper, applying modularity maximization, which was the method proposed in that paper, SSC, and OSC. 
Note that these are all algorithms for fitting a PABM. 
While our methods performed worse than modularity maximization, they were much faster. 

NOTE: DBLP = computer science citation network, "Digital Bibliography and Library Project"

In another example, we looked at the Karnataka villages data. 
These are household visitation networks of three villages.
Here, all three methods achieve comparable performance. 

NOTE: Karnataka = state in southwestern India

The work I presented is joint work with my advisors Michael Trosset and Minh Tang, and it was recently published in JCGS. 
You can also view all of the code on GitHub, and I am working on an R package for orthogonal spectral clustering, and its current state is also on GitHub. 

So the story thus far has been that we have three easy to understand yet widely applicable block models for modeling networks with community structure, and we have another model called the generalized random dot product graph, and the connection between them is that block models are generalized random dot product graphs with various community-wise latent structures. 
This makes community detection via fitting a block model to a network an easy and intuitive task--we first estimate the latent vectors via ASE, then we apply the appropriate clustering method for the particular structure for the block model of interest. 
This approach is also a good way to determine which type of block model is most appropriate for the data--we can simply look at the structure of the estimated latent vectors and choose the corresponding block model. 

In our final section, we will talk about more general models with community structure. 
We call these manifold block models, and the reason for this name will hopefully become clear in the next few slides. 

So first, a motivating example. 
Here is a graph that represents brain areas and connections of the macaque monkey. 
Two brain functions were mapped onto each area, one for visual responses and another for tactile responses. 
At first, this appears to be a good candidate for analysis as a block model, since it is an unweighted and undirected graph with community structure. 
However, the ASE shows that none of our clustering methods will work here. 
In particular, the ASE suggests nonlinear latent structure. 
A new type of block model is required for this analysis. 

So here is our definition for the manifold block model. 
As in the GRDPG, we start with a latent space, but we suppose that the latent vectors are restricted to K manifolds M_1, ..., M_K which are described by continuous functions g_1, ..., g_K. 
The latent vectors are then drawn as follows: 
First, the community labels are drawn from a categorical distribution. 
Then the index points t_1 to t_n are drawn from some distribution on the unit hypercube. 
Finally, each latent vector x_i is g_{z_i}(t_i), where g_{z_i}. 
Within this presentation, we only consider one dimensional manifolds, which are described by curves. 

The ASE can approximate the original latent structure, so while our previous methods will not work in this setting, there is still some hope that we can identify the communities. 

The method we propose for this setting is called K curves clustering. 
This is based on K means clustering, except instead of estimating means for each cluster, we estimate curves. 
The procedure is as follows.
First, we compute the ASE of an adjacency matrix. 
Then we initialize the community labels, for example, randomly, or via some other clustering method.
Then for each label, we fit a curve by specifying some functional form and adjusting the parameters of that function by minimizing the squared loss. 
Then once we fit the K curves, we reassign the labels based on distance to each curve. 

Like with K-means clustering, K-curves clustering can be sensitive to initialization, and like how K-means clustering makes an implicit assumption about how the data are distributed, the choice of the types of curves is an assumption on how the communities are structured in the latent space. 
In this example, we generated a manifold block model in which the communities map to two quadratic curves, as in the previous example, and we applied K-curves clustering on the ASE using various functional forms for the curves and various initializations. 
On the left, we started with a random initialization and assumed that the curves were quadratic, and we were able to recover the original latent curves, up to rotation. 
In the middle, we again assumed that the curves are quadratic, but we started with labels assigned via normalized spectral clustering, and we obtained something that looks plausible but we know to be "incorrect" from our knowledge of the original latent configuration. 
On the right, we again start with spectral clustering, but we fit cubic curves to the embedding, which again looks plausible and actually achieves a lower loss than the "true" solution, even though we know that this is "incorrect" based on the original latent configuration. 

So I mentioned the loss function for K-means clustering, and here it is spelled out. 
In effect, we are choosing labels and curves such that the distances between the embedding vectors and their assigned curves is small. 
And so based on the choice of curve, we can have different solutions that minimize the loss. 
Our theoretical result then needs some additional help. 
In this theorem, we first assume that the curves are of some fixed order R and we already know for each community at least R+1 vertices that belong to it. 
Then for larger and larger graphs, K-curves clustering will find labels and curves such that the loss function converges to zero with high probability. 

To illustrate our results, we have a couple of simulations.
In the first setup, we use the same two quadratic curves as before, and we start with 0, 4, and 8 known labels per community. 
In all three settings, the community detection error rate decays to zero. 
We also took a look at just one sample size, here n = 512, and we applied K-curves clustering many times with random initializations and visualized the results. 
We can see in this case that a little more than half the time, it was able to find something close to the true latent curves. 

In the second simulation example, we expanded both the number of curves and the dimensions of the latent space. 
Here, we have three quadratic latent curves in three dimensions. 
The results of these simulations are similar to that of the K=2 simulations in that the error rate appears to decay to zero as we draw larger and larger graphs. 

Turning back to our original macaque brain areas network example, K-curves clustering with quadratic curves appears to do a decent job of identifying the latent structure, although it misses one label that has a outsized effect on the estimated curves. 
In this example, K-curves clustering achieves an error rate of 4.4%.

We also applied K-curves clustering to the drosophila connectome network. 
The two graphs are for the left and right hemispheres of the brain. 
In these graphs, the vertices are neurons, the edges represent whether the neurons are connected, and the communities are the four neuron types. 
Again, K-curves clustering with quadratic curves appears to identify the structure reasonably well, and we get 72% and 84% accuracy for identifying the neuron type for the left and right hemispheres. 




