---
title: |
  | Semi-Parametric Manifold Clustering
output:
  pdf_document:
    keep_tex: true
    citation_package: natbib
fontsize: 11pt
# geometry: "left=1in,right=1in,top=1in,bottom=1in"
urlcolor: blue
header-includes:
- \usepackage{setspace}
- \usepackage{float}
- \usepackage{mathtools}
- \usepackage{natbib}
- \usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
- \setcitestyle{numbers,square,comma}
- \usepackage{verbatim}
- \usepackage{amsthm}
- \usepackage{comment}
bibliography: misc.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 3, 
                      fig.width = 3)
options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')

library(ggplot2)
import::from(magrittr, `%>%`)
```

```{r}
source('~/dev/pabm-grdpg/functions.R')
source('https://mtrosset.pages.iu.edu/Courses/675/graph.r')

laplacian.eigenmap <- function(W, d = 2, normalized = TRUE) {
  n <- nrow(W)
  if (normalized) {
    L <- normalized.laplacian(W)
  } else {
    L <- graph.laplacian(W)
  }
  L.eigen <- eigen(L, symmetric = TRUE)
  sweep(L.eigen$vectors[, seq(n - 1, n - d)], 2, 
        sqrt(L.eigen$values[seq(n - 1, n - d)]), `/`)
}

spectral.clustering <- function(W, K = 2, d = K, normalized = TRUE) {
  Y <- laplacian.eigenmap(W, d, normalized)
  kmeans(Y, K)$cluster
}
```

\newcommand{\diag}{\mathrm{diag}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\blockdiag}{\mathrm{blockdiag}}
\newcommand{\indep}{\stackrel{\mathrm{ind}}{\sim}}
\newcommand{\iid}{\stackrel{\mathrm{iid}}{\sim}}
\newcommand{\Bernoulli}{\mathrm{Bernoulli}}
\newcommand{\Betadist}{\mathrm{Beta}}
\newcommand{\Uniform}{\mathrm{Uniform}}
\newcommand{\BG}{\mathrm{BernoulliGraph}}
\newcommand{\Categorical}{\mathrm{Categorical}}
\newcommand{\Multinomial}{\mathrm{Multinomial}}
\newcommand{\RDPG}{\mathrm{RDPG}}
\newcommand{\GRDPG}{\mathrm{GRDPG}}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{example}
\newtheorem{example}{Example}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\as}{\stackrel{\mathrm{a.s.}}{\to}}

# Estimating Polynomial Curves

## Problem Setup

Let: 

* $T_1, ..., T_n \iid F$ with support $[0, 1]$. 
* $g(\cdot, \theta) : [0, 1] \mapsto \mathcal{X} \subset \mathbb{R}^d$. 
* $X_1, ..., X_n = g(T_1), ..., g(T_n)$

Assuming some parametric form of $g$ with parameters $\theta$, we want to find $\hat{\theta}$, some "reasonable" estimate for $\theta$. 
We observe $X_i$ but not $T_i$. 

For now, we limit $d = 2$ and $g$ to quadratic functions. 

```{r}
param.curve.quadr.2 <- function(t., 
                                beta10, beta11, beta12, 
                                beta20, beta21, beta22) {
  x1 <- beta10 + beta11 * t. + beta12 * t. ^ 2
  x2 <- beta20 + beta21 * t. + beta22 * t. ^ 2
  return(cbind(x1, x2))
}

estimate.one.t.quadr.2 <- function(x1, x2, 
                                   beta10, beta11, beta12, 
                                   beta20, beta21, beta22) {
  gamma0 <- beta10 * beta11 + beta20 * beta21 - beta11 * x1 - beta21 * x2
  gamma1 <- 2 * beta10 * beta12 + 2 * beta20 * beta22 + 
    beta11 ^ 2 + beta21 ^ 2 - 
    2 * beta12 * x1 - 2 * beta22 * x2
  gamma2 <- 3 * beta11 * beta12 + 3 * beta21 * beta22
  gamma3 <- 2 * (beta12 ^ 2 + beta22 ^ 2)
  roots <- polyroot(c(gamma0, gamma1, gamma2, gamma3))
  roots <- roots[abs(roots) == abs(Re(roots))]
  roots <- Re(roots)
  return(max(roots))
}

estimate.t.quadr.2 <- function(X, 
                               beta10, beta11, beta12, 
                               beta20, beta21, beta22) {
  apply(X, 1, function(x) {
    estimate.one.t.quadr.2(x[1], x[2], 
                           beta10, beta11, beta12, 
                           beta20, beta21, beta22)
  })
}

estimate.quadr.coefs.2 <- function(X, t.hat, intercept = TRUE) {
  sum.X <- colSums(X)
  sum.Xt <- colSums(sweep(X, 1, t.hat, `*`))
  sum.Xt2 <- colSums(sweep(X, 1, t.hat ^ 2, `*`))
  sum.t <- sum(t.hat)
  sum.t2 <- sum(t.hat ^ 2)
  sum.t3 <- sum(t.hat ^ 3)
  sum.t4 <- sum(t.hat ^ 4)
  
  if (intercept) {
    A.sub <- matrix(c(1, sum.t, sum.t2, 
                      sum.t, sum.t2, sum.t3, 
                      sum.t2, sum.t3, sum.t4), 
                    nrow = 3, ncol = 3)
    A <- Matrix::bdiag(A.sub, A.sub)
    
    b <- c(sum.X[1], sum.Xt[1], sum.Xt2[1], 
           sum.X[2], sum.Xt[2], sum.Xt2[2])
    return(solve(A, b))
  } else {
    A.sub <- matrix(c(sum.t2, sum.t3, sum.t3, sum.t4),
                    nrow = 2, ncol = 2)
    A <- Matrix::bdiag(A.sub, A.sub)
    b <- c(sum.Xt[1], sum.Xt2[1], sum.Xt[2], sum.Xt2[2])
    theta <- solve(A, b)
    return(c(0, theta[1:2], 0, theta[3:4]))
  }
}

estimate.quadr.curve.2 <- function(X, init.params, 
                                   intercept = TRUE,
                                   eps = 1e-6, maxit = 100) {
  if (missing(init.params)) {
    theta <- rnorm(6)
  } else {
    theta <- init.params
  }
  d.theta <- 1 / eps
  
  niter <- 0
  while (d.theta > eps) {
    theta.prev <- theta
    t.hat <- estimate.t.quadr.2(X,
                                theta[1], theta[2], theta[3],
                                theta[4], theta[5], theta[6])
    X.hat <- param.curve.quadr.2(t.hat, theta[1], theta[2], theta[3], 
                                 theta[4], theta[5], theta[6])
    theta <- estimate.quadr.coefs.2(X, t.hat, intercept)
    
    d.theta <- sum((theta - theta.prev) ^ 2) / sum(theta ^ 2)
    
    niter <- niter + 1
    if (niter >= maxit) {
      warning('failed to converge to a quadratic curve')
      break
    }
  }
  return(list(theta = theta, 
              X = X.hat,
              t = t.hat))
}
```

\begin{example}

Let $g(t) = (t^2, 2 t (1 - t) ) = (t^2, 2 t - 2 t^2)$. 
(This is the first two dimensions of the Hardy-Weinberg curve). 
Then $\theta = (0, 0, 1, 0, 2, -2)$.

```{r}
t. <- seq(0, 1, .01)
X <- param.curve.quadr.2(t., 0, 0, 1, 0, 2, -2)

ggplot() + 
  geom_line(aes(x = X[, 1], y = X[, 2])) + 
  theme_bw() + 
  labs(x = expression(x[1]), y = expression(x[2])) + 
  coord_fixed()
```

\end{example}

If we observe the $T_i$'s, then we can use a standard polynomial regression method to obtain $\hat{\theta}$. Since we do not observe them, the proposed iterative method is as follows:

1. Initialize $\hat{\theta}^{(0)}$ (e.g., by drawing from a probability distribution). 
2. Estimate each $\hat{t}_i^{(s)}$ by minimizing $L(t_i, \hat{\theta}^{(s)} | x_i) = L_i = \|x_i - g(t_i | \hat{\theta}^{(s)})\|^2$.
3. Compute each $\hat{x}_i^{(s)} = g(\hat{t}_i^{(s)} | \hat{\theta}^{(s)})$
4. Estimate $\hat{\theta}^{(s+1)}$ by minimizing $L(\{\hat{t}_i^{(s)}\}, \theta | X) = \sum_i \|x_i - g(\hat{t}_i^{(s)} | \theta)\|^2$.
5. Repeat steps 2-4 until convergence.

If we restrict $g$ to be polynomials, then steps (2) and (4) have closed-form solutions. 

\begin{example}

Write $g(t | \theta) = (g_1(t | \theta_1), ..., g_d(t | \theta_d))$ where $g_r(t | \theta_r)$ is the component of $g$ in the $r^{th}$ dimension and $\theta_r$ is the vector of parameters for the $r^{th}$ dimension. If $g_r$ are polynomials of degree $p$, then each $\theta_r$ contains up to $p + 1$ entries. 

Given the observed points $x_1, ..., x_n \in \mathbb{R}^d$ and their corresponding index points $t_1, ..., t_n \in \mathbb{R}$, we can find each $\hat{\theta}_r$ individually by $\hat{\theta}_r = A^{-1} b$ where $b \in \mathbb{R}^{p+1}$ and $b_k = \sum_i x_i t_i^k$ and $A \in \mathbb{R}^{(p+1) \times (p+1)}$ and $A_{kl} = \sum_i t^{(k-1) (l-1)}$.

On the other hand, if we have parameters $\theta$ but not the index points $t_i$, we can minimize each $t_i$ individually by finding the roots of a $p+1$ polynomial with coefficients that depend on $x_1, ..., x_n$ and $\theta$. 

In the following plot, we drew $n = 200$ points from the 2D H-W curve with $T_1, ..., T_n \iid Uniform(0, 1)$. 
The red line is the curve that was fit using the above method. 

\end{example}

```{r}
set.seed(314159)
n <- 200
t. <- runif(n)
X <- param.curve.quadr.2(t., 0, 0, 1, 0, 2, -2)

out <- estimate.quadr.curve.2(X)
thetahat <- out$theta
t.hat <- sort(out$t)
Xhat <- param.curve.quadr.2(t.hat, 
                            thetahat[1], thetahat[2], thetahat[3],
                            thetahat[4], thetahat[5], thetahat[6])

ggplot() + 
  geom_point(aes(x = X[, 1], y = X[, 2])) + 
  theme_bw() + 
  labs(x = expression(x[1]), y = expression(x[2])) + 
  coord_fixed() + 
  geom_line(aes(x = Xhat[, 1], y = Xhat[, 2]),
            colour = 'red')
```

One problem with this method is the parameterization of the curve is not unique. 

# Estimation with Noise

\begin{example}

In the next example, we draw $A \sim \RDPG(X)$ using the same H-W curve and sample size as above and estimate the true latent positions (up to rotation). 
In this example, we force the intercept terms to be zero.

\end{example}

```{r}
P <- X %*% t(X)
diag(P) <- 0
A <- draw.graph(P)
Y <- embedding(A, 2, 0)
```

```{r, cache = TRUE}
out <- estimate.quadr.curve.2(Y,
                              init.params = c(0, 0, 1, 0, 2, -2),
                              intercept = FALSE,
                              maxit = 200)
thetahat <- out$theta
t.hat <- sort(out$t)
Yhat <- param.curve.quadr.2(t.hat, 
                            thetahat[1], thetahat[2], thetahat[3],
                            thetahat[4], thetahat[5], thetahat[6])

ggplot() + 
  geom_point(aes(x = Y[, 1], y = Y[, 2])) + 
  theme_bw() + 
  labs(x = expression(x[1]), y = expression(x[2])) + 
  coord_fixed() + 
  geom_line(aes(x = Yhat[, 1], y = Yhat[, 2]),
            colour = 'red')
```

# Clustering

Next, suppose we have K curves parameterized by $g^{(k)}$, with points drawn along these curves. Then one possible clustering technique is as follows:

1. Assign an initial clustering (e.g., via spectral clustering).
2. Estimate the curve for each cluster.
3. Reassign the clusters by proximity to each cluster.
4. Repeat 2 and 3 until convergence.

\begin{example}

We again limit these to be quadratic functions in $\mathbb{R}^2$. 
Here, $K = 2$ and $n_1 = n_2 = 256$.

\end{example}

```{r}
compute.distances.quadr.2 <- function(X, theta) {
  t. <- estimate.t.quadr.2(X, 
                           theta[1], theta[2], theta[3], 
                           theta[4], theta[5], theta[6])
  X.hat <- param.curve.quadr.2(t., 
                               theta[1], theta[2], theta[3], 
                               theta[4], theta[5], theta[6])
  apply(X - X.hat, 1, function(x) sum(x ^ 2))
}

manifold.clustering.quadr.2 <- function(A, K = 2, 
                                        intercept = TRUE, 
                                        maxit = 100) {
  n <- nrow(A)
  
  z.hat <- spectral.clustering(A, K)
  X <- embedding(A, 2, 0)
  z.hat.prev <- sample(seq(K), n, replace = TRUE)
  
  niter <- 0
  
  while (!all(z.hat.prev == z.hat)) {
    z.hat.prev <- z.hat
    
    X1 <- X[z.hat == 1, ]
    X2 <- X[z.hat == 2, ]
    
    curve1 <- estimate.quadr.curve.2(X1, intercept = intercept)
    curve2 <- estimate.quadr.curve.2(X2, intercept = intercept)
    
    d1 <- compute.distances.quadr.2(X, curve1$theta)
    d2 <- compute.distances.quadr.2(X, curve2$theta)
    distances <- cbind(d1, d2)
    
    z.hat <- apply(distances, 1, which.min)
    
    niter <- niter + 1
    if (niter >= maxit) {
      warning('failed to converge to a clustering')
      break
    }
  }
  
  return(list(z = z.hat, 
              X = X,
              theta = list(curve1$theta, curve2$theta)))
}
```

```{r}
f1 <- function(t) {
  x1 <- t / sqrt(.5 + sqrt(5) / 2)
  x2 <- x1 ** 2
  return(cbind(x1, x2))
}

f2 <- function(t) {
  x2 <- t / sqrt(.5 + sqrt(5) / 2)
  x1 <- x2 ** 2
  return(cbind(x1, x2))
}

n1 <- n2 <- 2 ** 8
z <- c(rep(1, n1), rep(2, n2))
t1 <- sort(runif(n1))
t2 <- sort(runif(n2))
X1 <- f1(t1)
X2 <- f2(t2)
X <- rbind(X1, X2)

P <- X %*% t(X)
Xhat <- embedding(P, 2, 0)

manifold.out <- manifold.clustering.quadr.2(P, 2, intercept = FALSE)
zhat.manifold <- manifold.out$z
if (mean(z == zhat.manifold) < .5) {
  zhat.manifold <- 2 - zhat.manifold
}
t. <- seq(0, 1, .01)
theta1 <- manifold.out$theta[[1]]
theta2 <- manifold.out$theta[[2]]
Y1 <- param.curve.quadr.2(t., 
                          theta1[1], theta1[2], theta1[3], 
                          theta1[4], theta1[5], theta1[6])
Y2 <- param.curve.quadr.2(t.,
                          theta2[1], theta2[2], theta2[3],
                          theta2[4], theta2[5], theta2[6])
Y <- rbind(Y1, Y2)
z. <- c(rep(1, length(t.)),
        rep(2, length(t.)))

ggplot() + 
  geom_point(aes(x = Xhat[, 1], y = Xhat[, 2], colour = factor(z))) + 
  labs(x = expression(x[1]), y = expression(x[2]), colour = NULL) + 
  theme_bw() + 
  geom_line(aes(x = Y[, 1], y = Y[, 2],
                groups = factor(z.))) + 
  xlim(-1, 0) + ylim(-.5, .5) + 
  coord_fixed()
```

\begin{example}

Finally, we draw $A \sim \RDPG(X)$ from the above example and apply the clustering and curve fitting to the ASE of $A$.

\end{example}

```{r cache = TRUE, fig.width = 4, fig.height = 4}
set.seed(12345)

A <- draw.graph(P)
Xhat <- embedding(A, 2, 0)

manifold.out <- manifold.clustering.quadr.2(A, 2, intercept = FALSE)
zhat.manifold <- manifold.out$z
table(z, zhat.manifold)

plot(Xhat, col = z, asp = 1)
t. <- seq(-1, 1, .01)
theta1 <- manifold.out$theta[[1]]
theta2 <- manifold.out$theta[[2]]
Y1 <- param.curve.quadr.2(t., 
                          theta1[1], theta1[2], theta1[3], 
                          theta1[4], theta1[5], theta1[6])
Y2 <- param.curve.quadr.2(t.,
                          theta2[1], theta2[2], theta2[3],
                          theta2[4], theta2[5], theta2[6])
lines(Y1)
lines(Y2, col = 2)
```