---
title: |
  | Semi-Parametric Manifold Clustering
output:
  pdf_document:
    keep_tex: true
    citation_package: natbib
fontsize: 11pt
# geometry: "left=1in,right=1in,top=1in,bottom=1in"
urlcolor: blue
header-includes:
- \usepackage{setspace}
- \usepackage{float}
- \usepackage{mathtools}
- \usepackage{natbib}
- \usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
- \setcitestyle{numbers,square,comma}
- \usepackage{verbatim}
- \usepackage{amsthm}
- \usepackage{comment}
bibliography: misc.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 3, 
                      fig.width = 3)
options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')

library(ggplot2)
import::from(magrittr, `%>%`)
```

\newcommand{\diag}{\mathrm{diag}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\blockdiag}{\mathrm{blockdiag}}
\newcommand{\indep}{\stackrel{\mathrm{ind}}{\sim}}
\newcommand{\iid}{\stackrel{\mathrm{iid}}{\sim}}
\newcommand{\Bernoulli}{\mathrm{Bernoulli}}
\newcommand{\Betadist}{\mathrm{Beta}}
\newcommand{\Uniform}{\mathrm{Uniform}}
\newcommand{\BG}{\mathrm{BernoulliGraph}}
\newcommand{\Categorical}{\mathrm{Categorical}}
\newcommand{\Multinomial}{\mathrm{Multinomial}}
\newcommand{\RDPG}{\mathrm{RDPG}}
\newcommand{\GRDPG}{\mathrm{GRDPG}}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{example}
\newtheorem{example}{Example}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\as}{\stackrel{\mathrm{a.s.}}{\to}}

# Estimating Polynomial Curves

## Problem Setup

Let: 

* $T_1, ..., T_n \iid F$ with support $[0, 1]$. 
* $g(\cdot, \theta) : [0, 1] \mapsto \mathcal{X} \subset \mathbb{R}^d$. 
* $X_1, ..., X_n = g(T_1), ..., g(T_n)$

Assuming some parametric form of $g$ with parameters $\theta$, we want to find $\hat{\theta}$, some "reasonable" estimate for $\theta$. 
We observe $X_i$ but not $T_i$. 

For now, we limit $d = 2$ and $g$ to quadratic functions. 

```{r}
source('~/dev/pabm-grdpg/functions.R')
source('~/dev/dissertation/curve-fitting-methods/functions.R')
```

\begin{example}

Let $g(t) = (t^2, 2 t (1 - t) ) = (0 + 0 t + t^2, 0 + 2 t - 2 t^2)$. 
(This is the first two dimensions of the Hardy-Weinberg curve). 
Then $\theta = (0, 0, 1, 0, 2, -2)$.

```{r}
t. <- seq(0, 1, .01)
X <- param.curve.quadr.2(t., 0, 0, 1, 0, 2, -2)

ggplot() + 
  geom_line(aes(x = X[, 1], y = X[, 2])) + 
  theme_bw() + 
  labs(x = expression(x[1]), y = expression(x[2])) + 
  coord_fixed()
```

\end{example}

If we observe the $T_i$'s, then we can use a standard polynomial regression method to obtain $\hat{\theta}$. Since we do not observe them, the proposed iterative method is as follows:

1. Initialize $\hat{\theta}^{(0)}$ (e.g., randomly). 
2. Estimate each $\hat{t}_i^{(s)}$ by minimizing $L(t_i, \hat{\theta}^{(s)} | x_i) = L_i = \|x_i - g(t_i | \hat{\theta}^{(s)})\|^2$.
3. Compute each $\hat{x}_i^{(s)} = g(\hat{t}_i^{(s)} | \hat{\theta}^{(s)})$
4. Estimate $\hat{\theta}^{(s+1)}$ by minimizing $L(\{\hat{t}_i^{(s)}\}, \theta | X) = \sum_i \|x_i - g(\hat{t}_i^{(s)} | \theta)\|^2$.
5. Repeat steps 2-4 until convergence.

If we restrict $g$ to be polynomials, then steps (2) and (4) have closed-form solutions. 
Alternatively, we can estimate $g$ using more general forms, e.g., splines, which may require approximation.

\begin{example}

Write $g(t | \theta) = (g_1(t | \theta_1), ..., g_d(t | \theta_d))$ where $g_r(t | \theta_r)$ is the component of $g$ in the $r^{th}$ dimension and $\theta_r$ is the vector of parameters for the $r^{th}$ dimension. If $g_r$ are polynomials of degree $p$, then each $\theta_r$ contains up to $p + 1$ entries. 

Given the observed points $x_1, ..., x_n \in \mathbb{R}^d$ and their corresponding index points $t_1, ..., t_n \in \mathbb{R}$, we can find each $\hat{\theta}_r$ individually by $\hat{\theta}_r = A^{-1} b$ where $b \in \mathbb{R}^{p+1}$ and $b_k = \sum_i x_i t_i^k$ and $A \in \mathbb{R}^{(p+1) \times (p+1)}$ and $A_{kl} = \sum_i t^{(k-1) (l-1)}$.

On the other hand, if we have parameters $\theta$ but not the index points $t_i$, we can minimize each $t_i$ individually by finding the roots of a $p+1$ polynomial with coefficients that depend on $x_1, ..., x_n$ and $\theta$. 

In the following plot, we drew $n = 200$ points from the 2D H-W curve with $T_1, ..., T_n \iid Uniform(0, 1)$. 
The red line is the curve that was fit using the above method. 

\end{example}

```{r}
set.seed(314159)
n <- 200
t. <- runif(n)
X <- param.curve.quadr.2(t., 0, 0, 1, 0, 2, -2)

out <- estimate.quadr.curve.2(X)
thetahat <- out$theta
t.hat <- sort(out$t)
Xhat <- param.curve.quadr.2(t.hat, 
                            thetahat[1], thetahat[2], thetahat[3],
                            thetahat[4], thetahat[5], thetahat[6])

ggplot() + 
  geom_point(aes(x = X[, 1], y = X[, 2])) + 
  theme_bw() + 
  labs(x = expression(x[1]), y = expression(x[2])) + 
  coord_fixed() + 
  geom_line(aes(x = Xhat[, 1], y = Xhat[, 2]),
            colour = 'red')
```

One problem with this method is the parameterization of the curve is not unique. 

# Estimation with Noise

\begin{example}

In the next example, we draw $A \sim \RDPG(X)$ using the same H-W curve and sample size as above and estimate the true latent positions (up to rotation). 

\end{example}

```{r}
P <- X %*% t(X)
diag(P) <- 0
A <- draw.graph(P)
Y <- embedding(A, 2, 0)
```

```{r, cache = TRUE}
out1 <- estimate.quadr.curve.2(Y,
                              init.params = c(0, 0, 1, 0, 2, -2),
                              intercept = TRUE,
                              maxit = 200)
thetahat <- out1$theta
t.hat <- sort(out1$t)
Yhat <- param.curve.quadr.2(t.hat, 
                            thetahat[1], thetahat[2], thetahat[3],
                            thetahat[4], thetahat[5], thetahat[6])

ggplot() + 
  geom_point(aes(x = Y[, 1], y = Y[, 2])) + 
  theme_bw() + 
  labs(x = expression(x[1]), y = expression(x[2])) + 
  coord_fixed() + 
  geom_line(aes(x = Yhat[, 1], y = Yhat[, 2]),
            colour = 'red')
```

# Clustering

Next, suppose we have K curves parameterized by $g^{(k)}$, with points drawn along these curves. Then one possible clustering technique is as follows:

1. Assign an initial clustering (e.g., via spectral clustering).
2. Estimate the curve for each cluster.
3. Reassign the clusters by proximity to each curve.
4. Repeat 2 and 3 until convergence.

\begin{example}

We again limit these to be quadratic functions in $\mathbb{R}^2$. 
Here, $K = 2$ and $n_1 = n_2 = 256$. 

```{r}
f1 <- function(t) {
  x1 <- t / sqrt(.5 + sqrt(5) / 2)
  x2 <- x1 ** 2
  return(cbind(x1, x2))
}

f2 <- function(t) {
  x2 <- t / sqrt(.5 + sqrt(5) / 2)
  x1 <- x2 ** 2
  return(cbind(x1, x2))
}

n1 <- n2 <- 2 ** 8
z <- c(rep(1, n1), rep(2, n2))
t1 <- sort(runif(n1))
t2 <- sort(runif(n2))
X1 <- f1(t1)
X2 <- f2(t2)
X <- rbind(X1, X2)

P <- X %*% t(X)
Xhat <- embedding(P, 2, 0)

manifold.out <- manifold.clustering.quadr.2(P, 2, 
                                            intercept = FALSE, 
                                            method = 'polynomial')
zhat.manifold <- manifold.out$z
if (mean(z == zhat.manifold) < .5) {
  zhat.manifold <- 2 - zhat.manifold
}
t. <- seq(0, 1, .01)
theta1 <- manifold.out$theta[[1]]
theta2 <- manifold.out$theta[[2]]
Y1 <- param.curve.quadr.2(t., 
                          theta1[1], theta1[2], theta1[3], 
                          theta1[4], theta1[5], theta1[6])
Y2 <- param.curve.quadr.2(t.,
                          theta2[1], theta2[2], theta2[3],
                          theta2[4], theta2[5], theta2[6])
Y <- rbind(Y1, Y2)
z. <- c(rep(1, length(t.)),
        rep(2, length(t.)))

ggplot() + 
  geom_point(aes(x = Xhat[, 1], y = Xhat[, 2], colour = factor(z))) + 
  labs(x = expression(x[1]), y = expression(x[2]), colour = NULL) + 
  theme_bw() + 
  # geom_line(aes(x = Y[, 1], y = Y[, 2],
  #               groups = factor(z.))) + 
  xlim(-1, 0) + ylim(-.5, .5) + 
  coord_fixed()
```

Next, we draw $A \sim \RDPG(X)$ from the above example and apply the clustering and curve fitting to the ASE of $A$.
The community detection error in this example is 30\%.
In the following plot, the points are colored according to their true labels.

```{r cache = TRUE, fig.width = 3, fig.height = 3}
set.seed(12345)

A <- draw.graph(P)
Xhat <- embedding(A, 2, 0)

manifold.out <- manifold.clustering.quadr.2(A, 2, 
                                            intercept = FALSE,
                                            method = 'polynomial')
zhat.manifold <- manifold.out$z
# table(z, zhat.manifold)

# plot(Xhat, col = z, asp = 1)
t. <- seq(-1, 1, .01)
theta1 <- manifold.out$theta[[1]]
theta2 <- manifold.out$theta[[2]]
Y1 <- param.curve.quadr.2(t., 
                          theta1[1], theta1[2], theta1[3], 
                          theta1[4], theta1[5], theta1[6])
Y2 <- param.curve.quadr.2(t.,
                          theta2[1], theta2[2], theta2[3],
                          theta2[4], theta2[5], theta2[6])
Y <- rbind(Y1, Y2)
z. <- c(rep(1, length(t.)),
        rep(2, length(t.)))
# lines(Y1)
# lines(Y2, col = 2)

ggplot() + 
  geom_point(aes(x = Xhat[, 1], y = Xhat[, 2], colour = factor(z))) + 
  labs(x = expression(x[1]), y = expression(x[2]), colour = NULL) + 
  theme_bw() + 
  geom_path(aes(x = Y[, 1], y = Y[, 2],
                groups = factor(z.))) + 
  xlim(-.25, 1.25) + ylim(-.75, .75) + 
  coord_fixed()
```

\end{example}

A possibly more robust modification to this is to use Bezier curves for $g$. 
This is the same functional form as the polynomial curves we used before, but with different basis functions. 
For the following examples, we consider the quadratic Bezier curve, which has the following basis form:

$$
g(t | p) = (1 - t)^2 p_0 + 2 t (1 - t) p_1 + t^2 p_2
$$

Where as before, $g : [0, 1] \mapsto \mathbb{R}^d$ and $t \in [0, 1]$, and each $p_r \in \mathbb{R}^d$. 
Thus if we can fit each $p_r$, then the procedure is the same as before. 

Common methods for Bezier curve fitting do not require specification of $t_1, ..., t_n$. 
However, an ordering is required. 

The curve fitting method is then modified as follows:

1. Initialize $\hat{p}^{(0)}$ (e.g., randomly). 
2. Estimate each $\hat{t}_i^{(s)}$ by minimizing $L(t_i, \hat{p}^{(s)} | x_i) = L_i = \|x_i - g(t_i | \hat{p}^{(s)})\|^2$.
3. Reorder $X$ according to $t_1, ..., t_n$.
4. Estimate $\hat{p}^{(s)}$ for the reordered $X$ using Bezier curve fitting.
5. Repeat steps 2-4 until convergence.

Then this becomes step 2 of the clustering method outlined above. 

We can also estimate $\hat{p} = (T^\top T)^{-1} T^\top X$ where $T = \begin{bmatrix} t_1 & \cdots & t_R \end{bmatrix}$ and each $t_r = \binom{R}{r} (1-t)^{R-r} t^r$, where $R$ is the order of the Bezier curve, so using the method for estimating $t_1, ..., t_n$ as described before, we can iteratively solve for $\hat{p}$ in the same manner. If we use soft clustering (e.g., EM as opposed to $K$-means), we can also insert a diagonal matrix of weights $W_k$ to obtain

$$\hat{p}_k = (T^\top W_k T)^{-1} T^\top W_k X$$

where $X$ is the full data matrix, $T$ is the full model matrix, and $W_k = w_k$ with the entries of $w_k$ being the estimated probability of each $x_i$ being in cluster $k$. 

\begin{example}

```{r}
f1 <- function(t) {
  x1 <- t ^ 2
  x2 <- 2 * t - 2 * t ^ 2
  return(cbind(x1, x2))
}

f2 <- function(t) {
  x1 <- 2 * t - 2 * t ^ 2
  x2 <- 1 - 2 * t + t ^ 2
  return(cbind(x1, x2))
}
```

We have two intersecting curves, $g_1(t) = \begin{bmatrix} t^2 & 2 t (1 - t) \end{bmatrix}^\top$ and $g_2(t) = \begin{bmatrix} 2 t (1 - t) & (1 - t) ^ 2 \end{bmatrix}^\top$. $n_1 = n_2 = 128$ points are drawn uniformly from each.

```{r}
n1 <- 2 ** 7
n2 <- n1
n <- n1 + n2
z <- c(rep(1, n1), rep(2, n2))

a <- 1
b <- 1
t1 <- rbeta(n1, a, b)
t2 <- rbeta(n2, a, b)
T <- rbind(t1, t2)

X1 <- f1(t1)
X2 <- f2(t2)
X <- rbind(X1, X2)

ggplot() + 
  geom_point(aes(x = X[, 1], y = X[, 2], colour = factor(z))) + 
  coord_fixed() + 
  labs(x = expression(x[1]), y = expression(x[2]), colour = NULL) + 
  theme_bw()
```

We draw $A \sim \RDPG(X)$ and obtain the following ASE:

```{r}
P <- X %*% t(X)
diag(P) <- 0
A <- draw.graph(P)
Xhat <- embedding(A, 2, 0)
if (mean(Xhat[, 1]) < 0) Xhat[, 1] <- -Xhat[, 1]

ggplot() + 
  geom_point(aes(x = Xhat[, 1], y = Xhat[, 2], colour = factor(z))) + 
  coord_fixed() + 
  labs(x = expression(x[1]), y = expression(x[2]), colour = NULL) + 
  theme_bw()
```

Fitting two quadratic Bezier curves to these data yields a community detection error rate of 10\%. 
In the following plot, the points are labeled according to their estimated labels.

```{r cache = TRUE}
set.seed(314159265)
manifold.out <- manifold.clustering.quadr.2(A, 2, 
                                            # initialization = z,
                                            # initialization = 'spectral',
                                            initialization = 'random', 
                                            method = 'bezier')
zhat.manifold <- manifold.out$z

t. <- seq(-1, 1, .01)
theta1 <- manifold.out$theta[[1]]
theta2 <- manifold.out$theta[[2]]
Y1 <- param.curve.quadr.2(t., 
                          theta1[1], theta1[2], theta1[3], 
                          theta1[4], theta1[5], theta1[6])
Y2 <- param.curve.quadr.2(t.,
                          theta2[1], theta2[2], theta2[3],
                          theta2[4], theta2[5], theta2[6])
Y <- rbind(Y1, Y2)
z. <- c(rep(1, length(t.)),
        rep(2, length(t.)))

ggplot() + 
  geom_point(aes(x = Xhat[, 1], y = Xhat[, 2], colour = factor(zhat.manifold))) + 
  labs(x = expression(x[1]), y = expression(x[2]), colour = NULL) + 
  theme_bw() + 
  geom_path(aes(x = Y[, 1], y = Y[, 2],
                groups = factor(z.))) + 
  xlim(-.25, 1.25) + ylim(-.75, .75) + 
  coord_fixed()
```

\end{example}
